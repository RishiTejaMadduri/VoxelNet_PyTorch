{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import argparse\n",
    "\n",
    "import cv2\n",
    "cv2.setNumThreads(0)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from config import cfg\n",
    "from utils.utils import box3d_to_label\n",
    "from model.model import RPN3D\n",
    "from loader.kitti import KITTI as Dataset\n",
    "from loader.kitti import collate_fn\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description = 'training')\n",
    "\n",
    "parser.add_argument('--lr', type = float, default = 0.01, help = 'initial learning rate')\n",
    "parser.add_argument('--alpha', type = float, default = 1.5, help = 'alpha in loss function')\n",
    "parser.add_argument('--beta', type = float, default = 1, help = 'beta in loss function')\n",
    "\n",
    "parser.add_argument('--max_epoch', type = int, default = 1, help = 'max epoch')\n",
    "parser.add_argument('--batch_size', type = int, default = 1, help = 'batch size')\n",
    "parser.add_argument('--workers', type = int, default = 4)\n",
    "\n",
    "parser.add_argument('--summary_interval', type = int, default = 100, help = 'iter interval for training summary')\n",
    "parser.add_argument('--summary_val_interval', type = int, default = 200, help = 'iter interval for val summary')\n",
    "parser.add_argument('--val_epoch', type = int, default = 10, help = 'epoch interval for dump val data')\n",
    "\n",
    "parser.add_argument('--log_root', type = str, default = 'log')\n",
    "parser.add_argument('--log_name', type = str, default = 'train.txt')\n",
    "parser.add_argument('--tag', type = str, default = 'default', help = 'log tag')\n",
    "\n",
    "parser.add_argument('--print_freq', default = 20, type = int, help = 'print frequency')\n",
    "\n",
    "parser.add_argument('--resumed_model', type = str, default = '', help = 'if specified, load the specified model')\n",
    "parser.add_argument('--saved_model', type = str, default = 'kitti_{}.pth.tar')\n",
    "\n",
    "# For test data\n",
    "parser.add_argument('--output_path', type = str, default = './preds', help = 'results output dir')\n",
    "parser.add_argument('--vis', type = bool, default = True, help = 'set to True if dumping visualization')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def run():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(x) for x in cfg.GPU_AVAILABLE)\n",
    "\n",
    "    start_epoch = 0\n",
    "    global_counter = 0\n",
    "    min_loss = sys.float_info.max\n",
    "\n",
    "    # Build data loader\n",
    "    train_dataset = Dataset(os.path.join(cfg.DATA_DIR, 'training'), shuffle = True, aug = True, is_testset = False)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle = True, collate_fn = collate_fn,\n",
    "                                  num_workers = args.workers, pin_memory = False)\n",
    "\n",
    "    val_dataset = Dataset(os.path.join(cfg.DATA_DIR, 'validation'), shuffle = False, aug = False, is_testset = False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size = args.batch_size, shuffle = False, collate_fn = collate_fn,\n",
    "                                num_workers = args.workers, pin_memory = False)\n",
    "    val_dataloader_iter = iter(val_dataloader)\n",
    "\n",
    "    # Build model\n",
    "    model = RPN3D(cfg.DETECT_OBJ, args.alpha, args.beta)\n",
    "\n",
    "    # Resume model if necessary\n",
    "    if args.resumed_model:\n",
    "        model_file = os.path.join(save_model_dir, args.resumed_model)\n",
    "        if os.path.isfile(model_file):\n",
    "            checkpoint = torch.load(model_file)\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            global_counter = checkpoint['global_counter']\n",
    "            min_loss = checkpoint['min_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            print((\"=> Loaded checkpoint '{}' (epoch {}, global_counter {})\".format(\n",
    "                args.resumed_model, checkpoint['epoch'], checkpoint['global_counter'])))\n",
    "        else:\n",
    "            print((\"=> No checkpoint found at '{}'\".format(args.resumed_model)))\n",
    "\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Optimization scheme\n",
    "    optimizer = optim.Adam(model.parameters(), lr = args.lr)\n",
    "\n",
    "    lr_sched = optim.lr_scheduler.MultiStepLR(optimizer, [150])\n",
    "\n",
    "    # Init file log\n",
    "    log = open(os.path.join(args.log_root, args.log_name), 'a')\n",
    "\n",
    "    # Init TensorBoardX writer\n",
    "    summary_writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # train and validate\n",
    "    tot_epoch = start_epoch\n",
    "    for epoch in range(start_epoch, args.max_epoch):\n",
    "        # Learning rate scheme\n",
    "        lr_sched.step()\n",
    "\n",
    "        counter = 0\n",
    "        batch_time = time.time()\n",
    "\n",
    "        tot_val_loss = 0\n",
    "        tot_val_times = 0\n",
    "\n",
    "        for (i, data) in enumerate(train_dataloader):\n",
    "\n",
    "            model.train(True)   # Training mode\n",
    "\n",
    "            counter += 1\n",
    "            global_counter += 1\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Forward pass for training\n",
    "            _, _, loss, cls_loss, reg_loss, cls_pos_loss_rec, cls_neg_loss_rec = model(data)\n",
    "\n",
    "            forward_time = time.time() - start_time\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradient\n",
    "            clip_grad_norm_(model.parameters(), 5)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_time = time.time() - batch_time\n",
    "\n",
    "            if counter % args.print_freq == 0:\n",
    "                # Print training info\n",
    "                info = 'Train: {} @ epoch:{}/{} loss: {:.4f} reg_loss: {:.4f} cls_loss: {:.4f} cls_pos_loss: {:.4f} ' \\\n",
    "                       'cls_neg_loss: {:.4f} forward time: {:.4f} batch time: {:.4f}'.format(\n",
    "                    counter, epoch + 1, args.max_epoch, loss.item(), reg_loss.item(), cls_loss.item(), cls_pos_loss_rec.item(),\n",
    "                    cls_neg_loss_rec.item(), forward_time, batch_time)\n",
    "                info = '{}\\t'.format(time.asctime(time.localtime())) + info\n",
    "                print(info)\n",
    "\n",
    "                # Write training info to log\n",
    "                log.write(info + '\\n')\n",
    "                log.flush()\n",
    "\n",
    "            # Summarize training info\n",
    "            if counter % args.summary_interval == 0:\n",
    "                print(\"summary_interval now\")\n",
    "                summary_writer.add_scalars(str(epoch + 1), {'train/loss' : loss.item(),\n",
    "                                                            'train/reg_loss' : reg_loss.item(),\n",
    "                                                            'train/cls_loss' : cls_loss.item(),\n",
    "                                                            'train/cls_pos_loss' : cls_pos_loss_rec.item(),\n",
    "                                                            'train/cls_neg_loss' : cls_neg_loss_rec.item()}, global_counter)\n",
    "\n",
    "            # Summarize validation info\n",
    "            if counter % args.summary_val_interval == 0:\n",
    "                print('summary_val_interval now')\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.train(False)  # Validation mode\n",
    "\n",
    "                    val_data = next(val_dataloader_iter)    # Sample one batch\n",
    "\n",
    "                    # Forward pass for validation and prediction\n",
    "                    probs, deltas, val_loss, val_cls_loss, val_reg_loss, cls_pos_loss_rec, cls_neg_loss_rec = model(val_data)\n",
    "\n",
    "                    summary_writer.add_scalars(str(epoch + 1), {'validate/loss': loss.item(),\n",
    "                                                                'validate/reg_loss': reg_loss.item(),\n",
    "                                                                'validate/cls_loss': cls_loss.item(),\n",
    "                                                                'validate/cls_pos_loss': cls_pos_loss_rec.item(),\n",
    "                                                                'validate/cls_neg_loss': cls_neg_loss_rec.item()}, global_counter)\n",
    "\n",
    "                    try:\n",
    "                        # Prediction\n",
    "                        tags, ret_box3d_scores, ret_summary = model.module.predict(val_data, probs, deltas, summary = True)\n",
    "\n",
    "                        for (tag, img) in ret_summary:\n",
    "                            img = img[0].transpose(2, 0, 1)\n",
    "                            summary_writer.add_image(tag, img, global_counter)\n",
    "                    except:\n",
    "                        raise Exception('Prediction skipped due to an error!')\n",
    "\n",
    "                    # Add sampled data loss\n",
    "                    tot_val_loss += val_loss.item()\n",
    "                    tot_val_times += 1\n",
    "\n",
    "            batch_time = time.time()\n",
    "\n",
    "        # Save the best model\n",
    "        avg_val_loss = tot_val_loss / float(tot_val_times)\n",
    "        is_best = avg_val_loss < min_loss\n",
    "        min_loss = min(avg_val_loss, min_loss)\n",
    "        save_checkpoint({'epoch': epoch + 1, 'global_counter': global_counter, 'state_dict': model.module.state_dict(), 'min_loss': min_loss},\n",
    "                        is_best, args.saved_model.format(cfg.DETECT_OBJ))\n",
    "\n",
    "        # Dump test data every 10 epochs\n",
    "        if (epoch + 1) % args.val_epoch == 0:   # Time consuming\n",
    "            # Create output folder\n",
    "            os.makedirs(os.path.join(args.output_path, str(epoch + 1)), exist_ok = True)\n",
    "            os.makedirs(os.path.join(args.output_path, str(epoch + 1), 'data'), exist_ok = True)\n",
    "            os.makedirs(os.path.join(args.output_path, str(epoch + 1), 'log'), exist_ok=True)\n",
    "\n",
    "            if args.vis:\n",
    "                os.makedirs(os.path.join(args.output_path, str(epoch + 1), 'vis'), exist_ok = True)\n",
    "\n",
    "            model.train(False)  # Validation mode\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for (i, val_data) in enumerate(val_dataloader):\n",
    "\n",
    "                    # Forward pass for validation and prediction\n",
    "                    probs, deltas, val_loss, val_cls_loss, val_reg_loss, cls_pos_loss_rec, cls_neg_loss_rec = model(val_data)\n",
    "\n",
    "                    front_images, bird_views, heatmaps = None, None, None\n",
    "                    if args.vis:\n",
    "                        tags, ret_box3d_scores, front_images, bird_views, heatmaps = \\\n",
    "                            model.module.predict(val_data, probs, deltas, summary = False, vis = True)\n",
    "                    else:\n",
    "                        tags, ret_box3d_scores = model.module.predict(val_data, probs, deltas, summary = False, vis = False)\n",
    "\n",
    "                    # tags: (N)\n",
    "                    # ret_box3d_scores: (N, N'); (class, x, y, z, h, w, l, rz, score)\n",
    "                    for tag, score in zip(tags, ret_box3d_scores):\n",
    "                        output_path = os.path.join(args.output_path, str(epoch + 1), 'data', tag + '.txt')\n",
    "                        with open(output_path, 'w+') as f:\n",
    "                            labels = box3d_to_label([score[:, 1:8]], [score[:, 0]], [score[:, -1]], coordinate = 'lidar')[0]\n",
    "                            for line in labels:\n",
    "                                f.write(line)\n",
    "                            print('Write out {} objects to {}'.format(len(labels), tag))\n",
    "\n",
    "                    # Dump visualizations\n",
    "                    if args.vis:\n",
    "                        for tag, front_image, bird_view, heatmap in zip(tags, front_images, bird_views, heatmaps):\n",
    "                            front_img_path = os.path.join(args.output_path, str(epoch + 1), 'vis', tag + '_front.jpg')\n",
    "                            bird_view_path = os.path.join(args.output_path, str(epoch + 1), 'vis', tag + '_bv.jpg')\n",
    "                            heatmap_path = os.path.join(args.output_path, str(epoch + 1), 'vis', tag + '_heatmap.jpg')\n",
    "                            cv2.imwrite(front_img_path, front_image)\n",
    "                            cv2.imwrite(bird_view_path, bird_view)\n",
    "                            cv2.imwrite(heatmap_path, heatmap)\n",
    "\n",
    "            # Run evaluation code\n",
    "            cmd_1 = './eval/KITTI/launch_test.sh'\n",
    "            cmd_2 = os.path.join(args.output_path, str(epoch + 1))\n",
    "            cmd_3 = os.path.join(args.output_path, str(epoch + 1), 'log')\n",
    "            os.system(' '.join([cmd_1, cmd_2, cmd_3]))\n",
    "\n",
    "        tot_epoch = epoch + 1\n",
    "\n",
    "    print('Train done with total epoch:{}, iter:{}'.format(tot_epoch, global_counter))\n",
    "\n",
    "    # Close TensorBoardX writer\n",
    "    summary_writer.close()\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename = 'to_be_determined.pth.tar'):\n",
    "    torch.save(state, '%s/%s' % (save_model_dir, filename))\n",
    "    if is_best:\n",
    "        best_filename = filename.replace('.pth.tar', '_best.pth.tar')\n",
    "        shutil.copyfile('%s/%s' % (save_model_dir, filename), '%s/%s' % (save_model_dir, best_filename))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset_dir = cfg.DATA_DIR\n",
    "    train_dir = os.path.join(cfg.DATA_DIR, 'training')\n",
    "    val_dir = os.path.join(cfg.DATA_DIR, 'validation')\n",
    "    log_dir = os.path.join('./log', args.tag)\n",
    "    save_model_dir = os.path.join('./saved', args.tag)\n",
    "    os.makedirs(log_dir, exist_ok = True)\n",
    "    os.makedirs(save_model_dir, exist_ok = True)\n",
    "\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
